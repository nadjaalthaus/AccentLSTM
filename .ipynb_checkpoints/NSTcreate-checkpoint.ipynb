{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "373d1ddc-c1da-472a-ae24-5db70cca6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import NSTcorpus\n",
    "import NSTlexicon\n",
    "import NSTjulius\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64b292-2f11-4222-aa03-e3d12827a18b",
   "metadata": {},
   "source": [
    "Read lexicon information to create items for lexicon look-up\n",
    "Words (list of str) - words (length 927167)\n",
    "Pos (dict, key:lexical entry, val: POS - part of speech, e.g. NN)\n",
    "Morph (dict, key: word, val: morphological tags, e.g. 'AKT|PRS'\n",
    "Phon (dict, key: word, val: phonological transcript)\n",
    "NumSyll (dict, key: word, val: number of syllables)\n",
    "Syll (dict, key: word, val: list of transcribed syllables)\n",
    "Accent:\n",
    "Decomp:\n",
    "Base:\n",
    "Sem:\n",
    "SemInfo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8212c704-5601-41d8-b4dd-f384c1fe0bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading lex\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "Words, Pos, Morph, Phon, NumSyll, Syll, Accent, Decomp, Base, Sem, SemInfo = NSTlexicon.read_lexicon()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a94610-84f2-4d30-a48f-023914c6ca91",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sjuk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSemInfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msjuk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sjuk'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1df2e1e4-8bd2-4bae-9d41-196902149c4d",
   "metadata": {},
   "source": [
    "Read the NST database to obtain lists of wav files, transcripts, region of youth of the speaker (all length 502115)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f507d3-e18a-457b-ac83-947782a4c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdirlist=[\"/Volumes/Nephthys/NewNST/0467_sv_train_1\", \"/Volumes/Nephthys/NewNST/0467_sv_train_2\", \"/Volumes/Nephthys/NewNST/0467_sv_train_3\"]\n",
    "NSTwavfilenames, NSTtranscripts, NSTregionofyouth=NSTcorpus.read_corpus_raw(topdirlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4450d0-5108-4d1a-810a-b062ecde9949",
   "metadata": {},
   "source": [
    "We want to focus only on disyllabic nouns, i.e. where NumSyllDict has value 2 and Pos contains 'NN'. This leads to a subset of length 50404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "619ae8e8-85c2-4dcd-8f9c-9a3e01123ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RawDisyll=[k for k in NumSyll.keys() if NumSyll[k]==2 and 'NN' in Pos[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b91aef-7c44-4c88-9ae1-db47d323b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50404\n"
     ]
    }
   ],
   "source": [
    "print(len(RawDisyll))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e553dc-70c3-49dc-946b-a4f37dc0d57a",
   "metadata": {},
   "source": [
    "This is not yet specific enough and contains a lot of proper names, items with punctuation that might not work like alphanumeric words, and disyllabic items that aren't stressed on the first syllable like \"banan\" and \"poäng\", i.e. iambic stress. For the accent analysis we focus on trochaic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f86e9c2-208e-4051-a839-71a411e13606",
   "metadata": {},
   "outputs": [],
   "source": [
    "RawDisyll=[x for x in RawDisyll if x[0].islower()] #proper names capitalised, ignore those\n",
    "RawDisyll=[x for x in RawDisyll if x.isalpha()] #ignore items like a-lag a:et \n",
    "RawDisyll=[x for x in RawDisyll if not (len(Accent[x])==2 and Accent[x][0]==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f07806-f924-4618-8adf-badfb1befaea",
   "metadata": {},
   "source": [
    "This leaves 45887 items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd3b261-dac4-4abc-9946-5fd2a2a17812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45887"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RawDisyll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaffeb9-362f-412a-9eec-36eb3d19d698",
   "metadata": {},
   "source": [
    "Now, we want to gather information about individual occurrences. We first construct a list of token types (i.e. specific morphological forms) and dictionaries to look up frequencies. We limit this to items from the lexicon that occur at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6cf920-afbc-4dc4-a90a-78a07c04cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TypeFreq, LemmaFreq=NSTlexicon.TokenBaseFrequencies(NSTtranscripts, Base)\n",
    "Types=NSTlexicon.BaseFreqAboveX(NSTtranscripts, RawDisyll, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d153a16-8742-4271-a396-c666630ed664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8131\n"
     ]
    }
   ],
   "source": [
    "print(len(Types))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3e1b6-f100-4990-adc1-b5c96201260b",
   "metadata": {},
   "source": [
    "Selecting just word types that actually occur in the corpus, this boils down to 8131 types.\n",
    "Types list different morphological forms separately, i.e. this list contains 'apa' (monkey, sg.) and 'apor' (monkey, pl.) as two entries (This is important because the accent assignment may change between singular and plural, e.g. tiger1/tigrar2 so we don't want to conflate the recordings under a single lemma).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afe76b-ac64-470f-9f9b-af9f2100eff3",
   "metadata": {},
   "source": [
    "One final problem is tokens whose POS is ambiguous. For the present analysis we focus on nouns, but many Swedish word forms can be from multiple syntactic categories, such as 'vara', which could be noun ('product') or verb ('to be'). So, while 'vara' has a very high frequency count, most of these are in fact verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bcc7ec4-80ae-4e27-b37d-48348b00d369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NN', 'VB']\n",
      "5057\n"
     ]
    }
   ],
   "source": [
    "print(Pos['vara'])\n",
    "print(TypeFreq['vara'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa0b8c-9887-4fc6-aced-c831e30a18c1",
   "metadata": {},
   "source": [
    "We therefore use POS tagging (spacy) in order to filter out tokens that are in fact not nouns in their specific sentence context. To illustrate, an example where 'vara' is a noun, with the corresponding spacy output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e421a1d-edaa-4711-9d8b-12b9f6aa02b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En en DET\n",
      "vara vara NOUN\n",
      "är vara AUX\n",
      "av av ADP\n",
      "dålig dålig ADJ\n",
      "kvalite kvalite ADP\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp=spacy.load('sv_core_news_sm')\n",
    "doc = nlp(\"En vara är av dålig kvalite\")\n",
    "    \n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecfe8a-48f5-47a6-aafe-8de244b7b245",
   "metadata": {},
   "source": [
    "To do this, we first need to retrieve all transcripts of recordings for each token.\n",
    "We construct Type2Wav, a dictionary that lists for each key (type) all wav files with an occurrence, and Type2Trans, a dictionary that lists all transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a63b5ade-3a35-4b49-83d9-aedb2daea6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "def type_dictionaries(Types,Transcripts,Wavfiles):\n",
    "    index={}\n",
    "    for i in range(len(Transcripts)):\n",
    "        tStripped = strip_punctuation(Transcripts[i])\n",
    "        tSplit = tStripped.split()\n",
    "        for s in tSplit:\n",
    "            index.setdefault(s,[]).append(i)\n",
    "\n",
    "    type2wav={}\n",
    "    type2trans={}\n",
    "    \n",
    "    for type in Types:\n",
    "        for w in index[type]:\n",
    "            type2wav.setdefault(type,[]).append(Wavfiles[w])\n",
    "            type2trans.setdefault(type,[]).append(Transcripts[w])\n",
    "    \n",
    "    return type2wav, type2trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aee87c5-702c-4d38-b950-21f33aff3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "Type2Wav, Type2Trans=type_dictionaries(Types, NSTtranscripts, NSTwavfilenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c790d3f-021e-4447-aac3-8b88380491b1",
   "metadata": {},
   "source": [
    "\n",
    "Now we create a data frame with one row per token (occurrence) of each type, with columns ID (e.g. vingar1), Type (vingar), Base (vinge),Accent (2,0), AccList (e.g. can be [[2,0],[1,0]]) ,Morph (e.g. PLU|IND|NOM|UTR, can be a list ,POS (should always be NN at this stage because filtered earlier), WavFile (complete path), JuliusTranscript (complete path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afbabb-99d4-4ce4-b745-5341dc29f53d",
   "metadata": {},
   "source": [
    "In particular, we first pass any item that has more than one POS through spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17cd117-1d0a-43af-863c-1763c6622e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os.path\n",
    "import spacy\n",
    "\n",
    "#nlp = spacy.load(\"sv_pipeline\")\n",
    "nlp=spacy.load('sv_core_news_sm')   \n",
    "\n",
    "Tokens=[]\n",
    "IDList=[]\n",
    "BaseList=[]\n",
    "AccList=[]\n",
    "MorphList=[]\n",
    "POSList=[]\n",
    "WavList=[]\n",
    "JuliusList=[]\n",
    "TranscriptList=[]\n",
    "\n",
    "startidx=0\n",
    "#with open('/Volumes/Seth/NSTsvensktal/TokenDF_i.txt', 'r') as f:\n",
    "#    for line in f:\n",
    "#        startidx=int(line.strip())\n",
    "#print(startidx)\n",
    "\n",
    "for i in range(startidx,len(Types)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        TokenDF=pd.DataFrame({'ID':IDList, 'Token':Tokens,'Base':BaseList, 'Acc':AccList, 'POS':POSList, 'Transcript':TranscriptList })\n",
    "        #TokenDF.to_csv('/Volumes/Seth/NSTsvensktal/TokenDF'+str(i)+'.csv')\n",
    "        TokenDF.to_csv('/Volumes/Nephthys/NewNST/TokenDF'+str(i)+'.csv')\n",
    "        #with open('/Volumes/Seth/NSTsvensktal/TokenDF_i.txt', 'w') as f:\n",
    "        with open('/Volumes/Nephthys/NewNST/TokenDF_i.txt', 'w') as f:\n",
    "            f.write(str(i))\n",
    "        \n",
    "    t=Types[i]\n",
    "    \n",
    "    idcounter=1\n",
    "    if isinstance(Accent[t][0],list):\n",
    "        continue #exclude any words that have ambiguous accent assignment, i.e. where Acc[t] is a list of lists\n",
    "    if isinstance(Pos[t],list):\n",
    "        \n",
    "    \n",
    "        for j in range(len(Type2Trans[t])):\n",
    "            transcript=Type2Trans[t][j]\n",
    "            wavfile=Type2Wav[t][j]\n",
    "            doc=nlp(transcript)\n",
    "            tokens=[token.text for token in doc]\n",
    "            postags=[token.pos_ for token in doc]\n",
    "            idx=tokens.index(t)\n",
    "            thispos=postags[idx]\n",
    "            thisbase=Base[t]\n",
    "            if thispos=='NOUN': #use it\n",
    "                if i%100==0:\n",
    "                    print(transcript)\n",
    "                    print('spacy found NOUN for ' + t)\n",
    "                Tokens.append(t)\n",
    "                IDList.append(t+str(idcounter))\n",
    "                AccList.append(Accent[t])\n",
    "                POSList.append('NN')\n",
    "                WavList.append(wavfile)\n",
    "                if isinstance(thisbase,list):\n",
    "                    for b in thisbase:\n",
    "                        if b in Pos and 'NN' in Pos[b]:\n",
    "                            thisbase=b\n",
    "                            #print('breaking')\n",
    "                            break\n",
    "                    if not isinstance(thisbase,list):\n",
    "                        BaseList.append(thisbase)\n",
    "                    else:\n",
    "                        BaseList.append(thisbase[0])\n",
    "                    #print('did not break')\n",
    "                    #BaseList.append(thisbase[0])\n",
    "                else:\n",
    "                    BaseList.append(thisbase)\n",
    "                TranscriptList.append(transcript)\n",
    "                idcounter=idcounter+1\n",
    "            else:\n",
    "                if i%100==0:\n",
    "                    print('spacy found ' + thispos + ' for ' + t + '- discard.')\n",
    "    else:\n",
    "        if Pos[t]==\"NN\":\n",
    "            thisbase=Base[t]\n",
    "            if isinstance(thisbase, list):\n",
    "                for b in thisbase:\n",
    "                    if b in Pos and 'NN' in Pos[b]:\n",
    "                        thisbase=b\n",
    "                        #'NN break'\n",
    "                        break\n",
    "            for j in range(len(Type2Trans[t])):\n",
    "                transcript=Type2Trans[t][j]\n",
    "                wavfile=Type2Wav[t][j]\n",
    "                Tokens.append(t)\n",
    "                IDList.append(t+str(idcounter))\n",
    "                AccList.append(Accent[t])\n",
    "                POSList.append('NN')\n",
    "                BaseList.append(thisbase)\n",
    "                TranscriptList.append(transcript)\n",
    "                WavList.append(wavfile)\n",
    "                idcounter=idcounter+1\n",
    "                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "587267d5-1eb9-45f4-aa67-858f7ba27e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenDF=pd.DataFrame({'ID':IDList, 'Token':Tokens, 'Base':BaseList, 'Acc':AccList, 'POS':POSList, 'Transcript':TranscriptList , 'WavFile':WavList})\n",
    "TokenDF.to_csv('TokenDFspacywav.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0db33be-3214-43a4-b803-bbe65b3a2319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123681, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TokenDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1cec3b-a70f-4a2e-bccd-81051857cb21",
   "metadata": {},
   "source": [
    "Now, for every entry in TokenDF, we use the Julius speech recognizer with language models from KTH (https://www.speech.kth.se/asr/) to obtain word boundary timestamps for the recognized tokens. \n",
    "(From command line via NSTjulius.runjulius(wavlist)). We use NSTjulius.parseJulius() to check whether the output contains the target token. If not, the item is labelled as a 'miss'. We add a new column to the TokenDF which indicates either the filename of the recognised Julius output, or 'miss'.\n",
    "This procedure is successful for 123681 files, and fails for 20436 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8206686c-8093-4a4f-a9e5-36e10c3d3f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123681\n",
      "20436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sum(TokenDF['Julius']=='miss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbebee0-517d-4f8c-8a40-76832654d5da",
   "metadata": {},
   "source": [
    "Next we want to parse the julius output to find the cutpoints c1 and c2 for the relevant token for each item, then cut the wav file, and finally get the pitch for the cut wav file via praat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "719fb149-9b7d-49b2-ac4b-453580d6e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def cutwav(wavfilename, cutpoint1, cutpoint2, outfilename):\n",
    "    \n",
    "    subprocess.call([\"/Applications/Praat.app/Contents/MacOS/Praat\", \"--run\", \"cutwav.praat\", wavfilename, cutpoint1, cutpoint2, outfilename])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a475a-c816-4c17-9a58-bb803c2599fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NSTtools\n",
    "import os, re\n",
    "\n",
    "\n",
    "PitchFile=[]\n",
    "for i in range(TokenDF.shape[0]):\n",
    "    if i%100==0:\n",
    "        print('============')\n",
    "        print(i)\n",
    "    \n",
    "        print('============')\n",
    "        with open('progress.txt', 'w') as f:\n",
    "            f.write(str(i))\n",
    "        \n",
    "    \n",
    "\n",
    "    juliusfile=TokenDF.loc[i]['Julius']\n",
    "    b=TokenDF.loc[i]['Base']\n",
    "    \n",
    "    print(juliusfile)\n",
    "    \n",
    "    if os.path.isfile(juliusfile):\n",
    "        print('is path')\n",
    "        try:\n",
    "            [Parse,WIndex]=NSTjulius.parse_julius_output(Base, jfile=juliusfile)\n",
    "            cutpoint1=Parse[WIndex[b]][1]\n",
    "            cutpoint2=Parse[WIndex[b]][2]\n",
    "\n",
    "            filestemre=re.search('^(.*)\\.wav$', TokenDF.loc[i]['WavFile'])\n",
    "            filestem=filestemre.group(1)\n",
    "            tokenfile=filestem+'_'+TokenDF.loc[i]['ID']+'.wav'\n",
    "            print('cut...')\n",
    "            cutwav(TokenDF.loc[i]['WavFile'], str(int(cutpoint1)/100), str(int(cutpoint2)/100), tokenfile)\n",
    "            print('get pitch...')\n",
    "            pf=NSTtools.getpitch(tokenfile)\n",
    "            PitchFile.append(pf)\n",
    "            print('got pitch')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db997fc4-72b3-49f6-8f7b-ee231b398fb5",
   "metadata": {},
   "source": [
    "Add pitch file info to TokenDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65bdcd7-6b58-455c-bd39-0fd89f4bc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "\n",
    "TokenFiles=[]\n",
    "for i in range(TokenDF.shape[0]):\n",
    "    \n",
    "    if os.path.isfile(TokenDF.loc[i]['Julius']):\n",
    "        \n",
    "        stemre=re.search('(.*)\\.wav$', TokenDF.loc[i]['WavFile'])\n",
    "        stem=stemre.group(1)\n",
    "        tokenfile=stem+'_'+TokenDF.loc[i]['ID']+'.wav'\n",
    "        if os.path.isfile(tokenfile):\n",
    "            TokenFiles.append(tokenfile)\n",
    "        else:\n",
    "            TokenFiles.append(\"\")\n",
    "    else:\n",
    "        TokenFiles.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa7e23b-baa9-478a-971c-426592ceba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "\n",
    "PitchFiles=[]\n",
    "\n",
    "for i in range(TokenDF.shape[0]):\n",
    "    \n",
    "    if os.path.isfile(TokenDF.loc[i]['Julius']):\n",
    "        \n",
    "        stemre=re.search('(.*)\\.wav$', TokenDF.loc[i]['WavFile'])\n",
    "        stem=stemre.group(1)\n",
    "        pitchfilename=stem+'_'+TokenDF.loc[i]['ID']+'_pitch.txt'\n",
    "        \n",
    "        if os.path.isfile(pitchfilename):\n",
    "            PitchFiles.append(pitchfilename)\n",
    "        else:\n",
    "            \n",
    "            PitchFiles.append(\"\")\n",
    "    else:\n",
    "            PitchFiles.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33bd7d9-569f-4710-97e7-1b646dd8e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenDF['PitchFile']=PitchFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb2d17-57cc-49ec-a8aa-0d02da528ec8",
   "metadata": {},
   "source": [
    "The final stages of preprocessing, normalisation and smoothing, are done in R."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweDL",
   "language": "python",
   "name": "swedl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
